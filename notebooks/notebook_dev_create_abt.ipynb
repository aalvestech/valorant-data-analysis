{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ABT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Configs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, expr, count, sum, max, udf, dayofweek, date_format, when, mean, median\n",
    "from pyspark.sql.types import StringType\n",
    "sys.path.append('../src/')\n",
    "from aws.aws import Aws\n",
    "import io\n",
    "import pandas as pd \n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crating spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/30 17:41:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/30 17:41:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/30 17:41:51 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/05/30 17:41:51 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/05/30 17:41:51 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"ValorantDataAnalysis\").getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.access.key\", os.getenv('AWS_ACCESS_KEY_ID'))\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.secret.key\", os.getenv('AWS_SECRET_ACCESS_KEY'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating used classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws = Aws()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataframes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(bucket_name : str, folder_path : str) -> list:\n",
    "    \"\"\"\"\"\"\n",
    "    objects = aws.list_objetcs_s3(bucket_name, folder_path)\n",
    "\n",
    "\n",
    "    return objects\n",
    "\n",
    "def concat_files_s3(objects):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    json_files = [obj['Key'] for obj in objects]\n",
    "\n",
    "    for file in json_files:\n",
    "\n",
    "        response = aws.read_s3_v2(bucket_name='s3-tcc-fia-valorant', folder_path=file)\n",
    "        json_data = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    return io.StringIO(json_data)\n",
    "\n",
    "def read_spark(data_io):\n",
    "    \"\"\"\"\"\"\n",
    "    data_io = pd.read_csv(data_io)\n",
    "    return spark.createDataFrame(data_io)\n",
    "\n",
    "def create_dataframe(bucket_name : str, folder_path : str):\n",
    "    \"\"\"\"\"\"\n",
    "    objects = get_files(bucket_name, folder_path)\n",
    "    data_io = concat_files_s3(objects)\n",
    "    df = read_spark(data_io)\n",
    "\n",
    "    return df\n",
    "\n",
    "def save_dataframe_csv(bucket_name, folder_path, file_name, data, file_format):\n",
    "    # Convert DataFrame to CSV string\n",
    "    csv_buffer = io.StringIO()\n",
    "    data.toPandas().to_csv(csv_buffer, index=False)\n",
    "\n",
    "    # Retrieve CSV data from buffer\n",
    "    csv_buffer_value = csv_buffer.getvalue()\n",
    "\n",
    "    date = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = file_name + '_' + date + file_format\n",
    "    file_path = folder_path + file_name\n",
    "\n",
    "    # Write CSV string to S3\n",
    "    s3 = boto3.resource('s3')\n",
    "\n",
    "    try:\n",
    "        s3.Object(bucket_name, file_path).put(Body=csv_buffer_value)\n",
    "        print(f\"Data was written to S3://{bucket_name}/{folder_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
